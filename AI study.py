import numpy as np
import matplotlib.pyplot as plt
import random
from sklearn.datasets import *
from sklearn.model_selection import train_test_split

x = np.array([[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 50, 125, 44, 32, 66, 61, 123, 50, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 54, 125, 44, 32, 66, 61, 123, 49, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 48, 125, 44, 32, 66, 61, 123, 52, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 55, 125, 44, 32, 66, 61, 123, 51, 44, 57, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 53, 44, 97, 94, 50, 45, 52, 125, 44, 32, 66, 61, 123, 50, 44, 50, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 56, 44, 97, 94, 50, 45, 48, 125, 44, 32, 66, 61, 123, 57, 44, 50, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 54, 44, 97, 94, 50, 45, 55, 125, 44, 32, 66, 61, 123, 57, 44, 53, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 54, 44, 97, 94, 50, 43, 51, 125, 44, 32, 66, 61, 123, 52, 44, 53, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 54, 44, 97, 94, 50, 45, 49, 125, 44, 32, 66, 61, 123, 56, 44, 53, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 54, 44, 97, 94, 50, 45, 55, 125, 44, 32, 66, 61, 123, 50, 44, 53, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 43, 52, 125, 44, 32, 66, 61, 123, 51, 44, 54, 45, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 54, 125, 44, 32, 66, 61, 123, 54, 44, 54, 43, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 54, 125, 44, 32, 66, 61, 123, 54, 44, 54, 43, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 54, 125, 44, 32, 66, 61, 123, 54, 44, 54, 43, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 54, 125, 44, 32, 66, 61, 123, 54, 44, 54, 43, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 54, 125, 44, 32, 66, 61, 123, 54, 44, 54, 43, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 54, 125, 44, 32, 66, 61, 123, 54, 44, 54, 43, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 54, 125, 44, 32, 66, 61, 123, 54, 44, 54, 43, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 54, 125, 44, 32, 66, 61, 123, 54, 44, 54, 43, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63],[235, 145, 144, 32, 236, 167, 145, 237, 149, 169, 32, 65, 61, 123, 97, 43, 50, 44, 97, 94, 50, 45, 54, 125, 44, 32, 66, 61, 123, 54, 44, 54, 43, 97, 125, 236, 151, 144, 32, 235, 140, 128, 237, 149, 152, 236, 151, 172, 32, 65, 61, 66, 236, 157, 188, 32, 235, 149, 140, 44, 97, 236, 157, 152, 32, 234, 176, 146, 236, 157, 128, 63]])

y = np.array([2, -1, 2, 1, -3, 1, 3, -2, 2, -4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4])

# cancer = load_breast_cancer()
# x = cancer.data #입력값
# y = cancer.target

x_train_all, x_test, y_train_all, y_test = train_test_split(x, y, test_size = 0.1, random_state = 41)
x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, test_size = 0.1, random_state = 41)

train_mean = np.mean(x_train, axis = 0)
train_std = np.std(x_train, axis = 0)
x_train_scaled = (x_train - train_mean) / train_std
x_val_scaled = (x_val - train_mean) / train_std

learning_rate = 1

class SingleLayer:
    
    def __init__(self):
        self.w = None
        self.b = None
        self.losses = []
        self.w_history = []
        self.lr = learning_rate
        
    def forpass(self, x):
        z = np.sum(x * self.w) + self.b
        return z
    
    def backprop(self, x, err):
        w_grad = x * err
        b_grad = err
        return w_grad, b_grad
    
    def activation(self, z):
        z = np.clip(z, -100, None)
        a = 1 / (1 + np.exp(-z))
        return a
    
    def fit(self, x, y, epochs = 100):
        self.w = np.ones(x.shape[1])
        self.b = 0
        self.w_history.append(self.w.copy( ))
        for i in range(epochs):
            loss = 0
            indexes = np.random.permutation(np.arange(len(x)))
            for i in indexes:
                z = self.forpass(x[i])
                a = self.activation(z)
                err = -(y[i] - a)
                w_grad, b_grad = self.backprop(x[i], err)
                self.w -= self.lr * w_grad
                self.b -= b_grad
                self.w_history.append(self.w.copy( ))
                a = np.clip(a, 1e-10, 1-1e-10)
                loss += -(y[i] * np.log(a) + (1-y[i]) * np.log(1-a))
            self.losses.append(loss/len(y))
            
    def predict(self, x):
        z = [self.forpass(x_i) for x_i in x]
        return np.array(z) > 0
    
    def score(self, x, y):
        return np.mean(self.predict(x) == y)

layer = SingleLayer()
layer.fit(x_train_scaled, y_train)
print("정확도 : " + str(layer.score(x_val_scaled, y_val) * 100) + "%")

# u = []
# for i in range(1, 101, 1):
#     u.append(i)

# # plt.plot(u, layer.losses[0:])
# # plt.title('Model loss')
# # plt.xlabel('Epoch')
# # plt.ylabel('Loss')
# # plt.show()

